{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b3b892e",
   "metadata": {},
   "source": [
    "### Changelog \n",
    "\n",
    "- Separating **process_iplist()** output from VT calls to folders (todays_date)\n",
    "- changed output of **process_json()** FROM \"_parsed-combined\" to todays_date_parsed-combined in EACH RESPECTIVE FOLDER "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d748134",
   "metadata": {},
   "source": [
    "### Flow\n",
    "\n",
    "1. User edits and feeds ip.csv\n",
    "2. **process_iplist()** reads each and calls VT api \n",
    "3. Responses are stored in **\"downloaded_vtresponse\"** and seperated into folders by **respective dates** (DDMMYYYY)\n",
    "4. **process_json()** reads jsons in each folder and generates a compilation for that day in each folder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7663865c",
   "metadata": {},
   "source": [
    "### Questions / Todo List\n",
    "\n",
    "**Update** \n",
    "\n",
    "I have managed to get the scripts running from jupyter notebook but have the following questions\n",
    "\n",
    "(a) Receiving (via a web interface) either an individual or list of domais / IP addresses --> currently it's fed via CSV, should i create a front-end for people to upload their files?\n",
    "\n",
    "(b) Storing list of domains / IP address into a queue based list --> Is this the back-end of things? That is to say, this script runs in the back-end and whenever files come in from front-end it'll trigger the script?\n",
    "\n",
    "(c) Carryout enrichment --> Where does this \"processed in the previous X days\" come from? From what i understand, should i create a check such that when new information comes in, it will look at previous histories when the IP/Domain was checked, and continue / stop accordingly? \n",
    "\n",
    "(d) storing responses in disk and extracting subset into DB --> is there a specific subset you'd like? DB-wise I would prefer to try NoSQL as i have no experience with it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dca118d9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image not found\n",
      "processed_date not found\n",
      "{'whois_date': 1676767998, 'last_analysis_date': 1676767972, 'reputation': 0, 'last_analysis_stats': {'harmless': 87, 'malicious': 0, 'suspicious': 0, 'undetected': 0, 'timeout': 0}, 'total_votes': {'harmless': 0, 'malicious': 0}, 'as_owner': 'GOOGLE-CLOUD-PLATFORM', 'country': 'US', 'asn': 396982, 'image': '', 'processed_date': '23022023'}\n",
      "last_analysis_date not found\n",
      "image not found\n",
      "processed_date not found\n",
      "{'whois_date': 1606940287, 'last_analysis_date': 1676767972, 'reputation': 0, 'last_analysis_stats': {'harmless': 87, 'malicious': 0, 'suspicious': 0, 'undetected': 0, 'timeout': 0}, 'total_votes': {'harmless': 0, 'malicious': 0}, 'as_owner': 'Volia', 'country': 'UA', 'asn': 25229, 'image': '', 'processed_date': '23022023'}\n",
      "whois_date not found\n",
      "image not found\n",
      "processed_date not found\n",
      "{'whois_date': 1606940287, 'last_analysis_date': 1677050350, 'reputation': 0, 'last_analysis_stats': {'harmless': 87, 'malicious': 0, 'suspicious': 0, 'undetected': 0, 'timeout': 0}, 'total_votes': {'harmless': 0, 'malicious': 0}, 'as_owner': 'Init7 (Switzerland) Ltd.', 'country': 'CH', 'asn': 13030, 'image': '', 'processed_date': '23022023'}\n",
      "completed\n"
     ]
    }
   ],
   "source": [
    "import base64\n",
    "import hashlib\n",
    "import json\n",
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import datetime\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "json_template_ip = {\n",
    "    \n",
    "    \"whois_date\": \"\",\n",
    "    \"last_analysis_date\": \"\",\n",
    "    \"reputation\": \"\",\n",
    "    \"last_analysis_stats\": \"\",\n",
    "    \"total_votes\": \"\",\n",
    "    \"as_owner\": \"\",\n",
    "    \"country\": \"\",\n",
    "    \"asn\": \"\",\n",
    "    \"image\":\"\",\n",
    "    \"processed_date\":\"\"\n",
    "       \n",
    "}\n",
    "\n",
    "API_KEY = '0d9fdb6e32d74b9d12e3d894309531838c3aabe8d66b049fd3a7976fbedf2c68'  #@param  {type: \"string\"}\n",
    "    \n",
    "\n",
    "\n",
    "def process_iplist(filename_to_process, columnIndex):\n",
    "    \n",
    "    with open(filename_to_process + \".csv\", newline='') as inputfile:\n",
    "\n",
    "        if not os.path.exists(\"downloaded_vtresponse\"):\n",
    "                os.makedirs(\"downloaded_vtresponse\")\n",
    "\n",
    "        with open(filename_to_process + \"_tracker.csv\", 'w', newline='') as outputfile:\n",
    "            ip_list = csv.reader(inputfile, delimiter=',')\n",
    "            output_writer = csv.writer(outputfile, delimiter=',', quotechar='\"', quoting=csv.QUOTE_MINIMAL)\n",
    "\n",
    "            counter = 0\n",
    "\n",
    "            for row in ip_list:\n",
    "\n",
    "                if counter == 0:\n",
    "                    output_writer.writerow(row + [\"Time Run\"])\n",
    "                    outputfile.flush()\n",
    "                    counter += 1\n",
    "                    continue\n",
    "\n",
    "                print(\"Processing: #\" + str(counter) + \" - \" + row[columnIndex])\n",
    "\n",
    "                #Get an IP address Report\n",
    "                r = requests.get(\"https://www.virustotal.com/api/v3/ip_addresses/\"+row[0], headers={\"x-apikey\":API_KEY})\n",
    "\n",
    "                # TODO: Make generalised and incorporate timestamp in foldername\n",
    "                now = datetime.datetime.now()\n",
    "                dt_string = now.strftime(\"%d%m%Y\")\n",
    "                \n",
    "                if not os.path.exists(\"downloaded_vtresponse/\" + dt_string):\n",
    "                    os.makedirs(\"downloaded_vtresponse/\" + dt_string)\n",
    "#                 with open(\"downloaded_vtresponse/\" + row[columnIndex] +\".json\", \"w\") as outfile:\n",
    "                with open(\"downloaded_vtresponse/\" + dt_string + \"/\" + row[columnIndex] +\".json\", \"w\") as outfile:\n",
    "\n",
    "                    outfile.write(r.text)\n",
    "\n",
    "                output_writer.writerow(row + [datetime.datetime.now()])\n",
    "\n",
    "                outputfile.flush()\n",
    "\n",
    "                counter += 1\n",
    "                time.sleep(16)\n",
    "\n",
    "\n",
    "def process_json_folder(folder_to_process,json_template):\n",
    "    \n",
    "    # Get DDMMYYYY to input later\n",
    "    now = datetime.datetime.now()\n",
    "    dt_string = now.strftime(\"%d%m%Y\")\n",
    "\n",
    "    # Usual Folder: downloaded_vtresponse\n",
    "    combined_df = pd.DataFrame()\n",
    "    \n",
    "    \n",
    "    for filename in os.listdir(folder_to_process):\n",
    "        f = os.path.join(folder_to_process, filename)\n",
    "    \n",
    "#         print(\"f:\", f)\n",
    "        # check if it is a file\n",
    "        if os.path.isfile(f) and f[-5:]==\".json\":\n",
    "\n",
    "            # Opening JSON file\n",
    "            f = open(f)\n",
    "#             print(f)\n",
    "\n",
    "            # returns JSON object as\n",
    "            # a dictionary\n",
    "            data = json.load(f)\n",
    "#             print(data)\n",
    "\n",
    "            # load JSON template\n",
    "            new_row = json_template\n",
    "\n",
    "            # populate fields in JSON template\n",
    "            for key in new_row:\n",
    "                try:\n",
    "            #         print(loaded_json['data']['attributes'][key]) \n",
    "                    new_row[key] = data['data']['attributes'][key]\n",
    "\n",
    "                except: \n",
    "                    print(key,\"not found\")\n",
    "                    if key == \"processed_date\":\n",
    "                        new_row[key] = dt_string\n",
    "                    \n",
    "            print(new_row)\n",
    "            df_result = pd.json_normalize(new_row)\n",
    "            combined_df = pd.concat([combined_df, df_result], ignore_index=True, sort=False)\n",
    "    \n",
    "    \n",
    "#     print(combined_df)\n",
    "    now = datetime.datetime.now()\n",
    "    dt_string = now.strftime(\"%d%m%Y\")\n",
    "    \n",
    "    combined_df.to_csv(folder_to_process + '/' + dt_string + '_parsed-combined.csv')\n",
    "    \n",
    "\n",
    "\n",
    "# Process the list of IPs (CSVs ok but IP must be x column in the list)\n",
    "# process_iplist(\"Combined_TestKits_BotIPs\", 0)\n",
    "# process_iplist(\"ip\", 0)\n",
    "\n",
    "\n",
    "# Process the downloaded VT JSONs\n",
    "# process_json_folder(\"downloaded_vtresponse_10Jan_combinedFull5k\")\n",
    "process_json_folder(\"downloaded_vtresponse/23022023\",json_template_ip)\n",
    "\n",
    "\n",
    "print(\"completed\")\n",
    "exit(0)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7159acef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5bed90cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_template_general = {\n",
    "    \n",
    "    \"DNS\": \"\", \n",
    "    \"Whois\": \"\",\n",
    "    \"whois_date\": \"\", ## CONVERT FROM EPOCH TO USER FRIENDLY DATE\n",
    "    \"last_analysis_date\": \"\",\n",
    "    \"creation_date\": \"\",\n",
    "    \"reputation\": \"\",\n",
    "    \"registrar\": \"\",\n",
    "    \"last_analysis_stats\": \"\",  ## SEPERATE INTO 5 COLUMNS?\n",
    "    \"last_https_certificate\": \"\",\n",
    "    \"categories\": \"\",\n",
    "    \"total_votes\": \"\",\n",
    "    \"as_owner\": \"\",\n",
    "    \"country\": \"\",\n",
    "    \"asn\": \"\",\n",
    "    \"download_archived_page\":\"\",\n",
    "    \"image\":\"\",\n",
    "    \"processed_date\":\"\"     ## OWN FIELD TO CHECK X+7 DAYS\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "d8767b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_template_ip = {\n",
    "    \n",
    "    \"whois_date\": \"\",\n",
    "    \"last_analysis_date\": \"\",\n",
    "    \"reputation\": \"\",\n",
    "    \"last_analysis_stats\": \"\",\n",
    "    \"total_votes\": \"\",\n",
    "    \"as_owner\": \"\",\n",
    "    \"country\": \"\",\n",
    "    \"asn\": \"\",\n",
    "    \"image\":\"\",\n",
    "    \"processed_date\":\"\"\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3af73069",
   "metadata": {},
   "outputs": [],
   "source": [
    "json_template_domain = {\n",
    "    \n",
    "    \"DNS\": \"\", \n",
    "    \"Whois\": \"\",\n",
    "    \"whois_date\": \"\",\n",
    "    \"last_analysis_date\": \"\",\n",
    "    \"creation_date\": \"\",\n",
    "    \"reputation\": \"\",\n",
    "    \"registrar\": \"\",\n",
    "    \"last_analysis_stats\": \"\",\n",
    "    \"last_https_certificate\": \"\",\n",
    "    \"categories\": \"\",\n",
    "    \"total_votes\": \"\",\n",
    "    \"download_archived_page\":\"\",\n",
    "    \"image\":\"\",\n",
    "    \"processed_date\":\"\"\n",
    "    \n",
    "    \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1707db0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'whois_date': 1,\n",
       " 'last_analysis_date': 1,\n",
       " 'reputation': 1,\n",
       " 'last_analysis_stats': 1,\n",
       " 'total_votes': 1,\n",
       " 'as_owner': 1,\n",
       " 'country': 1,\n",
       " 'asn': 1,\n",
       " 'image': 1}"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "for key in json_template_ip:\n",
    "    json_template_ip[key] = 1 \n",
    "    \n",
    "json_template_ip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c31f8920",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "string indices must be integers",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [21]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m data \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(f)\n\u001b[0;32m      4\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys()\n\u001b[1;32m----> 5\u001b[0m \u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mdata\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mtype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mip_address\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: string indices must be integers"
     ]
    }
   ],
   "source": [
    "import json\n",
    "f = open(\"downloaded_vtresponse/22022023/34.123.194.52.json\")\n",
    "data = json.load(f)\n",
    "data['data'].keys()\n",
    "data['data']['type']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "670b3af1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "image not found\n",
      "processed_date not found\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'whois_date': 1676767998,\n",
       " 'last_analysis_date': 1676767972,\n",
       " 'reputation': 0,\n",
       " 'last_analysis_stats': {'harmless': 87,\n",
       "  'malicious': 0,\n",
       "  'suspicious': 0,\n",
       "  'undetected': 0,\n",
       "  'timeout': 0},\n",
       " 'total_votes': {'harmless': 0, 'malicious': 0},\n",
       " 'as_owner': 'GOOGLE-CLOUD-PLATFORM',\n",
       " 'country': 'US',\n",
       " 'asn': 396982,\n",
       " 'image': '',\n",
       " 'processed_date': '23022023'}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import datetime\n",
    "\n",
    "\n",
    "now = datetime.datetime.now()\n",
    "dt_string = now.strftime(\"%d%m%Y\")\n",
    "    \n",
    "f = open(\"downloaded_vtresponse/22022023/34.123.194.52.json\")\n",
    "\n",
    "new_row = json_template_ip \n",
    "loaded_json = json.load(f)\n",
    "                \n",
    "\n",
    "for key in new_row:\n",
    "    try:\n",
    "#         print(loaded_json['data']['attributes'][key]) \n",
    "        new_row[key] = loaded_json['data']['attributes'][key]\n",
    "\n",
    "    except: \n",
    "        print(key,\"not found\")\n",
    "        if key == \"processed_date\":\n",
    "            new_row[key] = dt_string\n",
    "            \n",
    "new_row"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fcfa5a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "utc: 2023-02-19 08:53:18\n",
      "sgt: 2023-02-19 08:53:18-07:00\n"
     ]
    }
   ],
   "source": [
    "import datetime\n",
    "from dateutil import tz\n",
    "\n",
    "epoch_time = 1676767998\n",
    "\n",
    "date_time = datetime.datetime.fromtimestamp( epoch_time )  \n",
    "\n",
    "print(\"utc:\", date_time)\n",
    "\n",
    "to_zone = tz.gettz('US/Mountain')\n",
    "new_date_time = date_time.replace(tzinfo=to_zone)\n",
    "\n",
    "\n",
    "print(\"sgt:\", new_date_time)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333cfbbd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
